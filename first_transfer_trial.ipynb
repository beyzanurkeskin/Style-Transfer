{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":310927,"sourceType":"datasetVersion","datasetId":130081},{"sourceId":675926,"sourceType":"datasetVersion","datasetId":341166}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T10:34:02.505637Z","iopub.execute_input":"2024-05-01T10:34:02.506824Z","iopub.status.idle":"2024-05-01T10:34:02.512851Z","shell.execute_reply.started":"2024-05-01T10:34:02.506784Z","shell.execute_reply":"2024-05-01T10:34:02.511612Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"artworks=\"/kaggle/input/best-artworks-of-all-time\"\nimagenet=\"/kaggle/input/imagenet\"","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:36:56.257435Z","iopub.execute_input":"2024-05-01T10:36:56.258904Z","iopub.status.idle":"2024-05-01T10:36:56.264128Z","shell.execute_reply.started":"2024-05-01T10:36:56.258852Z","shell.execute_reply":"2024-05-01T10:36:56.262913Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport os\n\n# Check TensorFlow version\nprint(\"TensorFlow version:\", tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:49.753392Z","iopub.execute_input":"2024-05-01T10:34:49.754309Z","iopub.status.idle":"2024-05-01T10:35:03.373031Z","shell.execute_reply.started":"2024-05-01T10:34:49.754253Z","shell.execute_reply":"2024-05-01T10:35:03.371904Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-05-01 10:34:51.686318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-01 10:34:51.686484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-01 10:34:51.825650: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow version: 2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 3: Implementing the Neural Style Transfer (NST) model\n\n# Import necessary modules\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\n\n# Define the NST model function\ndef build_nst_model():\n    # Load the pre-trained VGG16 model (without the top layers)\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    \n    # Choose intermediate layers for content and style representations\n    content_layers = ['block4_conv2']\n    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n    \n    # Output the content and style representations\n    content_outputs = [base_model.get_layer(layer).output for layer in content_layers]\n    style_outputs = [base_model.get_layer(layer).output for layer in style_layers]\n    \n    # Combine content and style outputs\n    model_outputs = content_outputs + style_outputs\n    \n    # Create the model\n    return Model(inputs=base_model.input, outputs=model_outputs)\n\n# Build the NST model\nnst_model = build_nst_model()\nnst_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:38:27.876119Z","iopub.execute_input":"2024-05-01T10:38:27.877223Z","iopub.status.idle":"2024-05-01T10:38:31.251880Z","shell.execute_reply.started":"2024-05-01T10:38:27.877180Z","shell.execute_reply":"2024-05-01T10:38:31.250857Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,995,072\u001b[0m (38.13 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,995,072</span> (38.13 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,995,072\u001b[0m (38.13 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,995,072</span> (38.13 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Step 4: Training the Neural Style Transfer (NST) model\n\n# Define loss functions\ndef content_loss(content, target):\n    return K.mean(K.square(content - target))\n\ndef gram_matrix(input_tensor):\n    channels = int(input_tensor.shape[-1])\n    a = K.reshape(input_tensor, (-1, channels))\n    n = K.shape(a)[0]\n    gram = K.dot(a, K.transpose(a)) / tf.cast(n, tf.float32)\n    return gram\n\ndef style_loss(style, target):\n    style_gram = gram_matrix(style)\n    target_gram = gram_matrix(target)\n    return K.mean(K.square(style_gram - target_gram))\n\n# Define total variation loss to maintain spatial coherence\ndef total_variation_loss(x):\n    a = K.square(x[:, :-1, :-1, :] - x[:, 1:, :-1, :])\n    b = K.square(x[:, :-1, :-1, :] - x[:, :-1, 1:, :])\n    return K.mean(K.pow(a + b, 1.25))\n\n# Define hyperparameters\ncontent_weight = 0.025\nstyle_weight = 1.0\ntotal_variation_weight = 1e-6\n\n# Define optimizer\noptimizer = Adam(learning_rate=0.02)\n\n# Define function to compute total loss\ndef compute_loss(model, loss_weights, init_image, content_target, style_target):\n    style_outputs = model(style_target)\n    print(style_outputs.shape)\n    content_outputs = model(content_target)\n    \n    style_loss_value = 0\n    content_loss_value = 0\n\n    content_weight, style_weight = loss_weights\n    \n    # Compute content loss\n    for target_content, comb_content in zip(content_target, content_outputs):\n        content_loss_value += content_weight * content_loss(target_content, comb_content)\n    \n    # Compute style loss\n    for target_style, comb_style in zip(style_target, style_outputs):\n        style_loss_value += style_weight * style_loss(target_style, comb_style)\n    \n    # Compute total variation loss\n    total_variation_loss_value = total_variation_weight * total_variation_loss(init_image)\n    \n    # Total loss\n    total_loss = content_loss_value + style_loss_value + total_variation_loss_value\n    return total_loss, style_loss_value, content_loss_value, total_variation_loss_value\n\n# Define function to compute gradients and apply them using an optimizer\n@tf.function()\ndef compute_grads(cfg):\n    with tf.GradientTape() as tape:\n        all_loss = compute_loss(**cfg)\n    grads = tape.gradient(all_loss[0], cfg['init_image'])\n    optimizer.apply_gradients([(grads, cfg['init_image'])])\n    return all_loss\n\n# Training loop\ndef train_nst(model, content_image, style_image, epochs=10, steps_per_epoch=100):\n    # Initialize generated image as content image\n    init_image = tf.Variable(content_image)\n    \n    # Define target content and style representations\n    content_target = model(content_image)\n    style_target = model(style_image)\n    \n    # Define configuration for loss computation\n    cfg = {\n        'model': model,\n        'loss_weights': (content_weight, style_weight),\n        'init_image': init_image,\n        'content_target': content_target,\n        'style_target': style_target\n    }\n    \n    # Training loop\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        for step in range(steps_per_epoch):\n            loss_values = compute_grads(cfg)\n            if step % 50 == 0:\n                print(f\"Step {step + 1}, Total Loss: {loss_values[0]}\")\n    \n    return init_image\n\nstyle_image_path = \"/kaggle/input/best-artworks-of-all-time/images/images/Francisco_Goya/Francisco_Goya_1.jpg\"\nstyle_image = keras.preprocessing.image.load_img(style_image_path, target_size=(224, 224))\nstyle_image = keras.preprocessing.image.img_to_array(style_image)\nstyle_image = np.expand_dims(style_image, axis=0)\nstyle_image = keras.applications.vgg16.preprocess_input(style_image)\n\ncontent_image_path ='/kaggle/input/imagenet/imagenet/train/ILSVRC2012_val_00005001.JPEG'\ncontent_image = keras.preprocessing.image.load_img(content_image_path, target_size=(224, 224))\ncontent_image = keras.preprocessing.image.img_to_array(content_image)\ncontent_image = np.expand_dims(content_image, axis=0)\ncontent_image = keras.applications.vgg16.preprocess_input(content_image)\n# Build the NST model\nnst_model = build_nst_model()\n\n# Convert datasets to TensorFlow tensors\ncontent_image = tf.convert_to_tensor(content_image)  # Replace with your content image\nstyle_image = tf.convert_to_tensor(style_image)  # Replace with your style image\n\n# Train the NST model\ngenerated_image = train_nst(nst_model, content_image, style_image, epochs=5, steps_per_epoch=100)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T11:19:39.594376Z","iopub.execute_input":"2024-05-01T11:19:39.594836Z","iopub.status.idle":"2024-05-01T11:19:41.326295Z","shell.execute_reply.started":"2024-05-01T11:19:39.594806Z","shell.execute_reply":"2024-05-01T11:19:41.324440Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m style_image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(style_image)  \u001b[38;5;66;03m# Replace with your style image\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train the NST model\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnst_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[19], line 90\u001b[0m, in \u001b[0;36mtrain_nst\u001b[0;34m(model, content_image, style_image, epochs, steps_per_epoch)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_epoch):\n\u001b[0;32m---> 90\u001b[0m     loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_fileespb4gk4.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_grads\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     all_loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(all_loss)[\u001b[38;5;241m0\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(cfg)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_image\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, ([(ag__\u001b[38;5;241m.\u001b[39mld(grads), ag__\u001b[38;5;241m.\u001b[39mld(cfg)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_image\u001b[39m\u001b[38;5;124m'\u001b[39m])],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n","File \u001b[0;32m/tmp/__autograph_generated_fileeglv42vy.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[0;34m(model, loss_weights, init_image, content_target, style_target)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m style_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_target\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(style_outputs)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m content_outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(content_target),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/input_spec.py:156\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_spec) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_34/7038979.py\", line 63, in compute_grads  *\n        all_loss = compute_loss(**cfg)\n    File \"/tmp/ipykernel_34/2818350052.py\", line 35, in compute_loss  *\n        style_outputs = model(style_target)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/layers/input_spec.py\", line 156, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer 'functional_9' expected 1 input(s). Received 6 instead.\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/tmp/ipykernel_34/7038979.py\", line 63, in compute_grads  *\n        all_loss = compute_loss(**cfg)\n    File \"/tmp/ipykernel_34/2818350052.py\", line 35, in compute_loss  *\n        style_outputs = model(style_target)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/layers/input_spec.py\", line 156, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer 'functional_9' expected 1 input(s). Received 6 instead.\n","output_type":"error"}]},{"cell_type":"code","source":"style_image.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T11:16:43.625409Z","iopub.execute_input":"2024-05-01T11:16:43.625818Z","iopub.status.idle":"2024-05-01T11:16:43.633358Z","shell.execute_reply.started":"2024-05-01T11:16:43.625789Z","shell.execute_reply":"2024-05-01T11:16:43.632131Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TensorShape([1, 224, 224, 3])"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model, optimizers\nimport numpy as np\nimport os\n\n# Function to preprocess images\ndef preprocess_image(image_path):\n    img = keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n    img = keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = keras.applications.vgg16.preprocess_input(img)\n    return img\n\n# Function to generate stylized images using NST model\ndef generate_stylized_images(nst_model, content_images, style_image):\n    stylized_images = []\n    for content_image in content_images:\n        generated_image = nst_model(tf.constant(content_image), tf.constant(style_image))\n        stylized_images.append(generated_image)\n    return stylized_images\n\n# Function to create a linear classifier model\ndef create_linear_classifier(input_shape, num_classes):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Flatten()(inputs)\n    x = layers.Dense(256, activation='relu')(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs, outputs)\n    return model\n\n# Load the pretrained VGG16 model\nbase_model = keras.applications.VGG16(weights='imagenet', include_top=False)\n\n# Define the NST model\ndef build_nst_model():\n    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n    content_layers = ['block4_conv2']\n\n    inputs = layers.Input(shape=(224, 224, 3))\n    x = inputs\n\n    style_outputs = []\n    content_outputs = []\n\n    for layer in base_model.layers:\n        x = layer(x)\n        if layer.name in style_layers:\n            style_outputs.append(x)\n        if layer.name in content_layers:\n            content_outputs.append(x)\n\n    model_outputs = style_outputs + content_outputs\n    return Model(inputs, model_outputs)\n\n# Load ImageNet dataset\nimagenet_dir = '/kaggle/input/imagenet/imagenet/train/'\nimage_files = os.listdir(imagenet_dir)\ncontent_images = [preprocess_image(os.path.join(imagenet_dir, file)) for file in image_files]\n\n# Load style image\nstyle_image_path = \"/kaggle/input/best-artworks-of-all-time/images/images/Francisco_Goya/Francisco_Goya_1.jpg\"\nstyle_image = preprocess_image(style_image_path)\n\n# Generate stylized images using NST model\nnst_model = build_nst_model()\nstylized_images = generate_stylized_images(nst_model, content_images, style_image)\n\n# Define labels for the images (example: for ImageNet, labels can be the image categories)\nlabels = [0] * len(content_images)\n\n# Concatenate the stylized images and original images\nall_images = np.concatenate((content_images, stylized_images), axis=0)\nlabels.extend([1] * len(stylized_images))\n\n# Shuffle the data\nidx = np.random.permutation(len(all_images))\nall_images = np.array(all_images)[idx]\nlabels = np.array(labels)[idx]\n\n# Split the dataset into training and testing sets\nsplit_idx = int(0.8 * len(all_images))\ntrain_images, test_images = all_images[:split_idx], all_images[split_idx:]\ntrain_labels, test_labels = labels[:split_idx], labels[split_idx:]\n\n# Create linear classifier model\nlinear_classifier = create_linear_classifier((7, 7, 512), 2)\n\n# Compile and train the model\nlinear_classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nlinear_classifier.fit(train_images, train_labels, epochs=5, batch_size=32, validation_data=(test_images, test_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T11:44:38.525143Z","iopub.execute_input":"2024-05-01T11:44:38.525580Z","iopub.status.idle":"2024-05-01T11:56:04.031653Z","shell.execute_reply.started":"2024-05-01T11:44:38.525550Z","shell.execute_reply":"2024-05-01T11:56:04.030057Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m style_image \u001b[38;5;241m=\u001b[39m preprocess_image(style_image_path)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Generate stylized images using NST model\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m nst_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_nst_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m stylized_images \u001b[38;5;241m=\u001b[39m generate_stylized_images(nst_model, content_images, style_image)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Define labels for the images (example: for ImageNet, labels can be the image categories)\u001b[39;00m\n","Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36mbuild_nst_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m content_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m style_layers:\n\u001b[1;32m     49\u001b[0m         style_outputs\u001b[38;5;241m.\u001b[39mappend(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/inspect.py:3107\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3105\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(parameters)\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m-> 3107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3110\u001b[0m         \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m         \u001b[38;5;66;03m# argument\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: too many positional arguments"],"ename":"TypeError","evalue":"too many positional arguments","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}